{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### messages_to_history, stream_graph, random_uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def get_role_from_messages(msg):\n",
    "    if isinstance(msg, HumanMessage):\n",
    "        return \"user\"\n",
    "    elif isinstance(msg, AIMessage):\n",
    "        return \"assistant\"\n",
    "    else:\n",
    "        return \"assistant\"\n",
    "\n",
    "\n",
    "def messages_to_history(messages):\n",
    "    return \"\\n\".join(\n",
    "        [f\"{get_role_from_messages(msg)}: {msg.content}\" for msg in messages]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "\n",
    "def stream_graph(\n",
    "    graph: CompiledStateGraph,\n",
    "    inputs: dict,\n",
    "    config: RunnableConfig,\n",
    "    node_names: List[str] = [],\n",
    "    callback: Callable = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LangGraph의 실행 결과를 스트리밍하여 출력하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        graph (CompiledStateGraph): 실행할 컴파일된 LangGraph 객체\n",
    "        inputs (dict): 그래프에 전달할 입력값 딕셔너리\n",
    "        config (RunnableConfig): 실행 설정\n",
    "        node_names (List[str], optional): 출력할 노드 이름 목록. 기본값은 빈 리스트\n",
    "        callback (Callable, optional): 각 청크 처리를 위한 콜백 함수. 기본값은 None\n",
    "            콜백 함수는 {\"node\": str, \"content\": str} 형태의 딕셔너리를 인자로 받습니다.\n",
    "\n",
    "    Returns:\n",
    "        None: 함수는 스트리밍 결과를 출력만 하고 반환값은 없습니다.\n",
    "    \"\"\"\n",
    "    prev_node = \"\"\n",
    "    for chunk_msg, metadata in graph.stream(inputs, config, stream_mode=\"messages\"):\n",
    "        curr_node = metadata[\"langgraph_node\"]\n",
    "\n",
    "        # node_names가 비어있거나 현재 노드가 node_names에 있는 경우에만 처리\n",
    "        if not node_names or curr_node in node_names:\n",
    "            # 콜백 함수가 있는 경우 실행\n",
    "            if callback:\n",
    "                callback({\"node\": curr_node, \"content\": chunk_msg.content})\n",
    "            # 콜백이 없는 경우 기본 출력\n",
    "            else:\n",
    "                # 노드가 변경된 경우에만 구분선 출력\n",
    "                if curr_node != prev_node:\n",
    "                    print(\"\\n\" + \"=\" * 50)\n",
    "                    print(f\"🔄 Node: \\033[1;36m{curr_node}\\033[0m 🔄\")\n",
    "                    print(\"- \" * 25)\n",
    "                print(chunk_msg.content, end=\"\", flush=True)\n",
    "\n",
    "            prev_node = curr_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def random_uuid():\n",
    "    return str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from operator import itemgetter\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "class RetrievalChain(ABC):\n",
    "    def __init__(self):\n",
    "        self.source_uri = None\n",
    "        self.k = 10\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_documents(self, source_uris):\n",
    "        \"\"\"loader를 사용하여 문서를 로드합니다.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_text_splitter(self):\n",
    "        \"\"\"text splitter를 생성합니다.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def split_documents(self, docs, text_splitter):\n",
    "        \"\"\"text splitter를 사용하여 문서를 분할합니다.\"\"\"\n",
    "        return text_splitter.split_documents(docs)\n",
    "\n",
    "    def create_embedding(self):\n",
    "        return OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    def create_vectorstore(self, split_docs):\n",
    "        return FAISS.from_documents(\n",
    "            documents=split_docs, embedding=self.create_embedding()\n",
    "        )\n",
    "\n",
    "    def create_retriever(self, vectorstore):\n",
    "        # MMR을 사용하여 검색을 수행하는 retriever를 생성합니다.\n",
    "        dense_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\", search_kwargs={\"k\": self.k}\n",
    "        )\n",
    "        return dense_retriever\n",
    "\n",
    "    def create_model(self):\n",
    "        return ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    def create_prompt(self):\n",
    "        return hub.pull(\"teddynote/rag-prompt-chat-history\")\n",
    "\n",
    "    @staticmethod\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\".join(docs)\n",
    "\n",
    "    def create_chain(self):\n",
    "        docs = self.load_documents(self.source_uri)\n",
    "        text_splitter = self.create_text_splitter()\n",
    "        split_docs = self.split_documents(docs, text_splitter)\n",
    "        self.vectorstore = self.create_vectorstore(split_docs)\n",
    "        self.retriever = self.create_retriever(self.vectorstore)\n",
    "        model = self.create_model()\n",
    "        prompt = self.create_prompt()\n",
    "        self.chain = (\n",
    "            {\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "                \"context\": itemgetter(\"context\"),\n",
    "                \"chat_history\": itemgetter(\"chat_history\"),\n",
    "            }\n",
    "            | prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "class PDFRetrievalChain(RetrievalChain):\n",
    "    def __init__(self, source_uri: Annotated[str, \"Source URI\"]):\n",
    "        self.source_uri = source_uri\n",
    "        self.k = 10\n",
    "\n",
    "    def load_documents(self, source_uris: List[str]):\n",
    "        docs :List[Document]= []\n",
    "        for source_uri in source_uris:\n",
    "            loader = PDFPlumberLoader(source_uri)\n",
    "            docs.extend(loader.load())\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def create_text_splitter(self):\n",
    "        return RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PDFRetrievalChain([\"*.pdf\"]).create_chain()\n",
    "\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            f\"<document><content>{doc.page_content}</content><source>{doc.metadata['source']}</source><page>{int(doc.metadata['page'])+1}</page></document>\"\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
